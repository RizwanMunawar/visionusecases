{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computer Vision Use Cases | Detection, Segmentation and More","text":"<p>Watch a quick overview of our top computer vision projects in action \ud83d\ude80</p>      Your browser does not support the video tag."},{"location":"#object-detection-advanced-applications","title":"Object Detection | Advanced Applications","text":"<p>Object detection is a pivotal computer vision technique that identifies and locates objects in images or videos. It integrates classification and localization to recognize object types and mark positions using bounding boxes. Common applications include autonomous driving, surveillance, and industrial automation.</p>        Object Detection Use Cases using Ultralytics YOLO"},{"location":"#featured-use-cases","title":"Featured Use Cases:","text":"<p>Explore key object detection projects we\u2019ve implemented, complete with technical insights:</p> <ul> <li>Waste Detection: \ud83d\ude80 Discover how cutting-edge object detection models like Ultralytics YOLO11 or YOLOv9 revolutionize waste detection for enhanced efficiency. </li> <li>Industrial Package Identification: \ud83d\udce6 Learn how to accurately detect packages in industrial settings using advanced models like Ultralytics YOLO11, YOLOv10, or Ultralytics YOLOv8. </li> </ul>"},{"location":"#object-tracking-monitoring-movement","title":"Object Tracking | Monitoring Movement","text":"<p>Object tracking monitors object movement across video frames. Starting with detection in the first frame, it tracks positions and interactions in subsequent frames. Common applications include surveillance, traffic monitoring, and sports analysis.</p>        Object Tracking Use Cases using Ultralytics YOLO"},{"location":"#featured-use-cases_1","title":"Featured Use Cases:","text":"<p>Explore our object tracking projects, showcasing technical depth and practical applications:</p> <ul> <li>Vehicle Tracking: \ud83d\ude97 Learn how to track vehicles with high accuracy using YOLOv10, YOLOv9, or YOLOv8, revolutionizing traffic monitoring and fleet management. </li> </ul>"},{"location":"#pose-estimation-key-point-analysis","title":"Pose Estimation | Key Point Analysis","text":"<p>Pose estimation predicts spatial positions of key points on objects or humans, enabling machines to interpret dynamics. This technique can be used in sports analysis, healthcare, and animation.</p>        Pose Estimation Use Cases using Ultralytics YOLO"},{"location":"#featured-use-cases_2","title":"Featured Use Cases:","text":"<p>Uncover our pose estimation projects with practical applications:</p> <ol> <li>Dog Pose Estimation: \ud83d\udc3e Learn how to estimate dog poses using Ultralytics YOLO11, unlocking new possibilities in animal behavior analysis. </li> </ol>"},{"location":"#object-counting-automation-at-scale","title":"Object Counting | Automation at Scale","text":"<p>Object counting identifies and tallies objects in images or videos. Leveraging detection or segmentation techniques, it\u2019s widely used in industrial automation, inventory tracking, and crowd management.</p>        Object Counting Use Cases using Ultralytics YOLO"},{"location":"#featured-use-cases_3","title":"Featured Use Cases:","text":"<p>Explore our object counting projects, complete with practical applications:</p> <ul> <li>Apples Counting on Conveyor Belt: \ud83c\udf4e Learn how to count apples with precision using Ultralytics YOLO models for better inventory management. </li> <li>Items Counting in Shopping Trolley: \ud83d\uded2 See how we track and count items in shopping trolleys with cutting-edge detection models, streamlining retail operations. </li> <li>Bread Counting on Conveyor Belt: \ud83c\udf5e Discover how to ensure accurate bread counts on conveyor belts with Ultralytics YOLO models, boosting production efficiency. </li> </ul>"},{"location":"#image-segmentation-precise-pixel-level-analysis","title":"Image Segmentation | Precise Pixel-Level Analysis","text":"<p>Image segmentation divides an image into meaningful regions to identify objects or areas of interest. Unlike object detection, it provides a precise outline of objects by labeling individual pixels. This technique is widely used in medical imaging, autonomous vehicles, and scene understanding.</p>        Instance Segmentation Use Cases using Ultralytics YOLO"},{"location":"#featured-use-cases_4","title":"Featured Use Cases:","text":"<p>Delve into our instance segmentation projects, featuring technical details and real-world applications:</p> <ul> <li>Brain Scan Segmentation: \ud83e\udde0 Learn how to segment brain scans with precision using models like Ultralytics YOLO11 or YOLOv8, revolutionizing medical imaging analysis. </li> </ul>"},{"location":"#faq","title":"FAQ","text":""},{"location":"#what-makes-ultralytics-yolo-models-unique","title":"What makes Ultralytics YOLO models unique?","text":"<p>Ultralytics YOLO models excel in real-time performance, high accuracy, and versatility across tasks like detection, tracking, segmentation, and counting. They are optimized for edge devices and seamlessly integrate into diverse workflows.</p>"},{"location":"#how-does-the-tracking-module-enhance-object-detection","title":"How does the tracking module enhance object detection?","text":"<p>The tracking module goes beyond detection by monitoring objects across video frames, providing trajectories and interactions. It's ideal for real-time applications like traffic monitoring, surveillance, and sports analysis.</p>"},{"location":"#can-the-object-counting-implementation-handle-dynamic-environments","title":"Can the Object Counting implementation handle dynamic environments?","text":"<p>Yes, the Object Counting implementation is designed for dynamic settings, like conveyor belts or crowded scenes, by accurately detecting and counting objects in real-time, ensuring operational efficiency.</p>"},{"location":"usecases/apple-counting/","title":"Apple Counting on Conveyor Belt using Ultralytics YOLO11","text":"<p>Accurate counting of apples in agricultural setups plays a significant role in yield estimation, supply chain optimization, and resource planning. Leveraging computer vision and AI, we can automate this process with impressive accuracy and efficiency.</p> Apple Counting on Conveyor Belt using Ultralytics YOLO11"},{"location":"usecases/apple-counting/#hardware-model-and-dataset-information","title":"Hardware, Model and Dataset Information","text":"<ul> <li>CPU: Intel\u00ae Core\u2122 i5-10400 CPU @ 2.90GHz.</li> <li>GPU: NVIDIA RTX 3050 for real-time processing tasks.</li> <li>RAM: 64 GB RAM and 1TB HardDisk</li> <li>Model: A fine-tuned Ultralytics YOLO11 model was utilized for object detection and counting.  </li> <li>Dataset: The dataset used in this project is proprietary and was annotated in-house by our team.</li> </ul>"},{"location":"usecases/apple-counting/#real-world-applications-for-fruit-counting","title":"Real-World Applications for Fruit Counting","text":"<ul> <li>Yield Estimation: Accurately counting fruits like apples, oranges, or bananas helps farmers forecast harvest sizes, allowing for better planning and resource allocation to meet market demands.</li> <li>Harvesting Optimization: Automated fruit counting aids in identifying the optimal harvest time, ensuring maximum yield and quality while reducing labor costs and wastage.</li> <li>Sorting and Grading: Fruit counting systems integrated with sorting lines improve efficiency in grading fruits by size, weight, or ripeness, directly impacting pricing and market appeal.</li> <li>Supply Chain Management: Real-time fruit counts enable precise tracking and packaging, streamlining logistics and reducing spoilage during transportation to markets or retailers.</li> </ul>"},{"location":"usecases/apple-counting/#social-resources","title":"Social Resources","text":"<ul> <li>LinkedIn Post: Revolutionizing Apple Counting with Ultralytics YOLO11</li> <li>Twitter Thread: Counting Apples in Agriculture</li> </ul>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/","title":"Real-Time Bird Tracking with Path Visualization Using Ultralytics YOLO11 \ud83e\udd85","text":"<p>Explore how to track birds and visualize their flight paths using Ultralytics YOLO11. This guide will show you how to detect birds, calculate flight paths, and visualize their movement using a custom-trained YOLO11 model.</p> Fig-1: Bird Tracking with Path Visualization Using Ultralytics YOLO11."},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#step-1-load-the-yolo-model-and-video","title":"Step 1: Load the YOLO Model and Video","text":"<p>We start by loading the fine-tuned YOLO model and preparing the video file for processing. The model used in this tutorial is already trained to detect birds efficiently in real-time.  </p> <pre><code>import cv2  \nimport numpy as np  \nfrom ultralytics import YOLO  \nfrom ultralytics.utils.plotting import Annotator  \n\n# Load the YOLO model for bird detection  \nmodel = YOLO(\"yolo11s.pt\")  \n\n# Load the video file  \ncap = cv2.VideoCapture(\"path/to/video/file.mp4\")  \n\n# Retrieve video properties like width, height, and frame rate  \nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH,  \n                                       cv2.CAP_PROP_FRAME_HEIGHT,  \n                                       cv2.CAP_PROP_FPS))  \n\n# Initialize video writers for saving original and tracking visualizations  \nvwriter = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), \n                          fps, (w, h))  \nvwriter1 = cv2.VideoWriter(\"birds_black.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), \n                           fps, (w, h))  \n</code></pre>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#step-2-start-video-processing","title":"Step 2: Start Video Processing","text":"<p>Now it's time to read the video frame by frame, preparing each frame for further processing while ensuring the video feed runs continuously until all frames are processed.</p> <pre><code>while cap.isOpened():  \n    success, im0 = cap.read()  # Read a video frame  \n    if not success:  \n        break  \n\n    ann = Annotator(im0, line_width=3)  # Initialize annotator for drawing  \n\n    # Create a blank frame for path visualization\n    empty_image = np.zeros_like(im0)  \n</code></pre>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#step-3-detect-and-track-birds","title":"Step 3: Detect and Track Birds","text":"<p>We will utilize the YOLO model loaded in memory during Step #01 to detect birds in the current frame, extracting bounding boxes, class labels, and unique tracking IDs for each detected bird.</p> <pre><code># Perform bird detection and tracking  \nresults = model.track(im0, persist=True)  \n\n# Extract bounding boxes, class labels, and tracking IDs  \nboxes = results[0].boxes.xyxy.cpu().numpy()  \nclss = results[0].boxes.cls.cpu().tolist()  \ntrack_ids = results[0].boxes.id.int().cpu().tolist()  \n</code></pre>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#step-4-visualize-bird-movement-paths","title":"Step 4: Visualize Bird Movement Paths","text":"<p>In this step, we will determine the centroid of each detected bird's bounding box, draw circles to represent their movement, and overlay tracking IDs on a blank frame for path visualization.</p> <pre><code>for box, cls, t_id in zip(boxes, clss, track_ids):  \n    # Calculate the centroid of the bounding box  \n    x1, y1, x2, y2 = box  \n    cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)  \n\n    # Draw a white circle at the centroid to represent bird movement  \n    circle_radius = 30  \n    cv2.circle(empty_image, (cx, cy), radius=circle_radius, \n               color=(255, 255, 255), thickness=-1)  \n\n    # Display tracking ID inside the circle for easy identification  \n    font_scale = 1.0  \n    font_thickness = 2  \n    text_size = cv2.getTextSize(str(t_id), cv2.FONT_HERSHEY_SIMPLEX, \n                                font_scale, font_thickness)[0]  \n    text_x = cx - text_size[0] // 2  \n    text_y = cy + text_size[1] // 2  \n    cv2.putText(empty_image, str(t_id), (text_x, text_y), \n                cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), \n                font_thickness)  \n\n    # Annotate the original frame with detection boxes  \n    ann.box_label(box, label=model.names[cls], color=(235, 219, 11))  \n</code></pre>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#step-5-save-and-display-frames","title":"Step 5: Save and Display Frames","text":"<p>Now it's time to display both the original and tracking visualization frames while saving them as video files for future reference and analysis.</p> <pre><code># Display the original frame  \ncv2.imshow(\"Original Frame\", im0)  \n\n# Save processed frames to video files  \nvwriter.write(im0)  \nvwriter1.write(empty_image)  \n\n# Stop if the user presses 'q'  \nif cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):  \n    break\n</code></pre>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#step-6-release-resources","title":"Step 6: Release Resources","text":"<p>Finally, we release all resources like video files and close display windows to complete the tracking process.  </p> <pre><code># Release all resources  \nvwriter.release()  \nvwriter1.release()  \ncap.release()  \ncv2.destroyAllWindows()  \n</code></pre>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#complete-code-in-one-block","title":"Complete Code in One Block","text":"<pre><code>import cv2  \nimport numpy as np  \nfrom ultralytics import YOLO  \nfrom ultralytics.utils.plotting import Annotator  \n\nmodel = YOLO(\"yolo11s.pt\")   # Load the YOLO model \ncap = cv2.VideoCapture(\"path/to/video/file.mp4\")   # Load the video \n\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH,  \n                                       cv2.CAP_PROP_FRAME_HEIGHT,  \n                                       cv2.CAP_PROP_FPS))  \n\n# Initialize video writers  \nvwriter = cv2.VideoWriter(\"output.avi\", \n                          cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))  \nvwriter1 = cv2.VideoWriter(\"birds_black.avi\", \n                           cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))  \n\n# Process video frames  \nwhile cap.isOpened():  \n    success, im0 = cap.read()  \n    if not success:  \n        break  \n\n    ann = Annotator(im0, line_width=3)  \n    empty_image = np.zeros_like(im0)  \n\n    # Detect and track birds  \n    results = model.track(im0, persist=True)  \n    boxes = results[0].boxes.xyxy.cpu().numpy()  \n    clss = results[0].boxes.cls.cpu().tolist()  \n    track_ids = results[0].boxes.id.int().cpu().tolist()  \n\n    for box, cls, t_id in zip(boxes, clss, track_ids):  \n        x1, y1, x2, y2 = box  \n        cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)  \n\n        # Draw tracking circles  \n        circle_radius = 30  \n        cv2.circle(empty_image, (cx, cy), radius=circle_radius, \n                   color=(255, 255, 255), thickness=-1)  \n\n        # Display tracking ID  \n        font_scale = 1.0  \n        font_thickness = 2  \n        text_size = cv2.getTextSize(str(t_id), cv2.FONT_HERSHEY_SIMPLEX, \n                                    font_scale, font_thickness)[0]  \n        text_x = cx - text_size[0] // 2  \n        text_y = cy + text_size[1] // 2  \n        cv2.putText(empty_image, str(t_id), (text_x, text_y), \n                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), \n                    font_thickness)  \n\n        # Annotate original frame  \n        ann.box_label(box, label=model.names[cls], color=(235, 219, 11))  \n\n    # Display frames  \n    cv2.imshow(\"Original Frame\", im0)  \n\n    # Save frames to output files  \n    vwriter.write(im0)  \n    vwriter1.write(empty_image)  \n\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):  \n        break  \n\n# Release resources  \nvwriter.release()  \nvwriter1.release()  \ncap.release()  \ncv2.destroyAllWindows()  \n</code></pre> <p>It's time to watch the Output \ud83d\ude80</p>      Your browser does not support the video tag."},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#real-world-applications","title":"Real World Applications","text":"<p>There are many real-world applications for this, but a few where this idea can be used are mentioned below:  </p> <ul> <li>Wildlife Monitoring: Study flight paths in real-time to monitor habitat usage, track species movement, and understand animal behavior for conservation purposes.  </li> <li>Ecological Research: Analyze migration patterns and seasonal movements to gain insights into bird populations and their responses to environmental changes.  </li> <li>Environmental Impact Assessment: Evaluate how ecological changes affect bird populations by analyzing flight data and habitat shifts over time.</li> </ul>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#faq","title":"FAQ","text":""},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#what-is-bird-tracking-with-path-visualization","title":"What is bird tracking with path visualization?","text":"<p>Bird tracking with path visualization involves detecting birds in videos or live streams, tracking their movements, and displaying flight paths visually using computer vision models like YOLO11.  </p>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#which-models-are-best-for-bird-tracking","title":"Which models are best for bird tracking?","text":"<p>Models like Ultralytics YOLO11, YOLOv10 and Ultralytics YOLOv8 are highly effective for bird detection and path visualization due to their real-time tracking capabilities.  </p>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#can-this-system-work-in-real-time","title":"Can this system work in real-time?","text":"<p>Yes, with the right hardware (GPU-accelerated systems) and optimized models, real-time bird tracking is achievable for ecological monitoring and wildlife research.  </p>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#what-are-the-real-world-applications-of-bird-tracking","title":"What are the real-world applications of bird tracking?","text":"<p>Bird tracking is used in wildlife monitoring, ecological research, environmental impact assessments, and even aviation safety to prevent bird-aircraft collisions.  </p>"},{"location":"usecases/bird-tracking-with-tracks-visualization-on-empty-image/#social-resources","title":"Social Resources","text":"<ul> <li>Join the Discussion on LinkedIn</li> </ul> <p>Start your bird tracking journey with YOLO11 today! \ud83d\ude80</p>"},{"location":"usecases/bread-counting/","title":"Bread Counting on Conveyor Belt Using Ultralytics YOLO11","text":"<p>Automating bread counting on conveyor belts enhances efficiency in bakeries, ensuring accurate packaging, minimizing wastage, and optimizing production workflows. Leveraging Ultralytics YOLO11 for object detection, this solution streamlines the bread production process with precision and speed.  </p>      Bread Counting on Conveyor Belt using Ultralytics YOLO11"},{"location":"usecases/bread-counting/#hardware-model-and-dataset-information","title":"Hardware, Model, and Dataset Information","text":"<ul> <li>CPU: Intel\u00ae Core\u2122 i5-10400 CPU @ 2.90GHz.  </li> <li>GPU: NVIDIA RTX 3050 for real-time processing of conveyor belt data.  </li> <li>RAM: 64 GB RAM and 1TB Hard Disk for seamless model execution and data handling.  </li> <li>Model: A fine-tuned Ultralytics YOLO11 model specifically optimized for bread detection and counting tasks.  </li> <li>Dataset: A proprietary dataset annotated in-house was used to train and validate the model for high accuracy.  </li> </ul>"},{"location":"usecases/bread-counting/#real-world-applications-for-bread-counting","title":"Real-World Applications for Bread Counting","text":"<ul> <li>Automated Packaging: Ensures accurate counting of bread loaves for packaging, reducing manual errors and speeding up the process.  </li> <li>Production Monitoring: Tracks production rates in real-time, enabling bakeries to optimize their workflows and meet demand efficiently.  </li> <li>Supply Chain Management: Provides precise inventory updates, ensuring seamless coordination between production, packaging, and delivery teams.  </li> </ul>"},{"location":"usecases/bread-counting/#social-resources","title":"Social Resources","text":"<ul> <li>LinkedIn Post: Automating Bread Counting with YOLO11 </li> <li>Twitter Thread: Bread Counting Revolution on Conveyor Belts</li> </ul>"},{"location":"usecases/cauli-flowers-analytics/","title":"CauliFlower Analytics","text":"<p>This documentation page is in progress</p>"},{"location":"usecases/computer-vision-project-workflow/","title":"Computer Vision Project Workflow","text":"<p>This documentation page is in progress</p>"},{"location":"usecases/crowd-density-estimation/","title":"Accurate Crowd Density Estimation Using Ultralytics YOLO11 \ud83c\udfaf","text":"<p>Discover how to utilize Ultralytics YOLO11 for accurate crowd density estimation. This guide will take you through a step-by-step implementation using a YOLO11-based system to measure and monitor crowd density in various environments, improving safety and event management capabilities.</p>"},{"location":"usecases/crowd-density-estimation/#system-specifications-used-for-this-implementation","title":"System Specifications Used for This Implementation","text":"<ul> <li>CPU: Intel\u00ae Core\u2122 i7-10700 CPU @ 2.90GHz for efficient processing.</li> <li>GPU: NVIDIA RTX 3060 for faster object detection.</li> <li>RAM &amp; Storage: 32 GB RAM and 512 GB SSD for optimal performance.</li> <li>Model: Pre-trained YOLO11 model for person detection.</li> <li>Dataset: Custom dataset for various crowd scenarios to fine-tune YOLO11 performance.</li> </ul>"},{"location":"usecases/crowd-density-estimation/#how-to-implement-crowd-density-estimation","title":"How to Implement Crowd Density Estimation","text":""},{"location":"usecases/crowd-density-estimation/#step-1-setup-and-model-initialization","title":"Step 1: Setup and Model Initialization","text":"<p>To get started, the code utilizes a pre-trained YOLO11 model for person detection. This model is loaded into the <code>CrowdDensityEstimation</code> class, which is designed to track individuals in a crowd and estimate crowd density in real time.</p>"},{"location":"usecases/crowd-density-estimation/#code-to-initialize-and-track-with-yolo11","title":"Code to Initialize and Track with YOLO11","text":"<pre><code>import cv2\nfrom estimator import CrowdDensityEstimation\n\ndef main():\n    estimator = CrowdDensityEstimation()  \n\n    # Open video capture (0 for webcam, or video file path)\n    cap = cv2.VideoCapture(\"path/to/video/file.mp4\")\n\n    # Get video properties for output\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter('crowd-density-estimation.mp4', \n                          fourcc, fps, (frame_width, frame_height))\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        processed_frame, density_info = estimator.process_frame(frame)\n        estimator.display_output(processed_frame, density_info)  # Display\n        out.write(processed_frame)  # Write output frame\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This setup captures frames from a video source, processes them using YOLO11 to detect people, and calculates crowd density.</p>"},{"location":"usecases/crowd-density-estimation/#step-2-real-time-crowd-detection-and-tracking","title":"Step 2: Real-Time Crowd Detection and Tracking","text":"<p>The core of the implementation relies on tracking individuals in each frame using the YOLO11 model and estimating the crowd density. This is achieved through a series of steps, which include detecting people, calculating density, and classifying the crowd level.</p>"},{"location":"usecases/crowd-density-estimation/#code-for-crowd-density-estimation","title":"Code for Crowd Density Estimation","text":"<p>The main class <code>CrowdDensityEstimation</code> includes the following functionality:</p> <ul> <li>Person Detection: Using YOLO11 to detect individuals in each frame.</li> <li>Density Calculation: Based on the number of detected persons relative to the frame area.</li> <li>Tracking: Visualization of tracking history for each detected person.</li> </ul> <pre><code># Install ultralytics package\n# pip install ultralytics\n\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom collections import defaultdict\n\nclass CrowdDensityEstimation:\n    def __init__(self, model_path='yolo11n.pt', conf_threshold=0.3):\n        self.model = YOLO(model_path)\n        self.conf_threshold = conf_threshold\n        self.track_history = defaultdict(lambda: [])\n        self.density_levels = {\n            'Low': (0, 0.2), # 0-0.2 persons/m\u00b2\n            'Medium': (0.2, 0.5), # 0.2-0.5 persons/m\u00b2\n            'High': (0.5, 0.8), # 0.5-0.8 persons/m\u00b2\n            'Very High': (0.8, float('inf')) # &gt;0.8 persons/m\u00b2\n        }\n\n    def extract_tracks(self, im0):\n        results = self.model.track(im0, persist=True, \n                                   conf=self.conf_threshold, \n                                   classes=[0])\n        return results\n\n    def calculate_density(self, results, frame_area):\n        if not results or len(results) == 0:\n            return 0, 'Low', 0\n\n        person_count = len(results[0].boxes)\n        density_value = person_count / frame_area * 10000  \n\n        density_level = 'Low'\n        for level, (min_val, max_val) in self.density_levels.items():\n            if min_val &lt;= density_value &lt; max_val:\n                density_level = level\n                break\n\n        return density_value, density_level, person_count\n</code></pre>"},{"location":"usecases/crowd-density-estimation/#step-3-visualizing-density-and-results","title":"Step 3: Visualizing Density and Results","text":"<p>Once density is calculated, the processed frame is annotated with information like density level, person count, and a tracking visualization. This enhances situational awareness by providing clear visual cues.</p>"},{"location":"usecases/crowd-density-estimation/#displaying-density-information-on-video-frames","title":"Displaying Density Information on Video Frames","text":"<pre><code>def display_output(self, im0, density_info):\n    density_value, density_level, person_count = density_info\n\n    cv2.rectangle(im0, (0, 0), (350, 150), (0, 0, 0), -1)\n\n    cv2.putText(im0, f'Density Level: {density_level}', (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n    cv2.putText(im0, f'Person Count: {person_count}', (10, 70),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n    cv2.putText(im0, f'Density Value: {density_value:.2f}', (10, 110),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n    # Display the frame\n    cv2.imshow('Crowd Density Estimation', im0)\n</code></pre>"},{"location":"usecases/crowd-density-estimation/#applications-of-crowd-density-estimation","title":"Applications of Crowd Density Estimation","text":"<p>1- Public Safety </p> <ul> <li>Early Warning System: Detecting unusual crowd formations.</li> <li>Emergency Response: Identifying areas of high density for quick intervention.</li> </ul> <p>2- Event Management </p> <ul> <li>Capacity Monitoring: Real-time tracking of crowd sizes in venues.</li> <li>Safety Compliance: Ensuring attendance stays within safe limits.</li> <li>Flow Analysis: Understanding movement patterns for better event planning.</li> </ul> <p>3- Urban Planning </p> <ul> <li>Space Utilization: Analyzing how people use public spaces.</li> <li>Infrastructure Planning: Designing facilities based on crowd patterns.</li> </ul>"},{"location":"usecases/crowd-density-estimation/#explore-more","title":"Explore More","text":"<ul> <li>Contribute to Ultralytics Solutions</li> <li>Author's LinkedIn Post Discussion </li> </ul> <p>Unlock the potential of advanced crowd monitoring using YOLO11 and streamline operations for various sectors! \ud83d\ude80</p>"},{"location":"usecases/items-counting/","title":"Item Counting in Trolleys for Smart Shopping Using Ultralytics YOLO11","text":"<p>Efficiently counting items in shopping trolleys can transform the retail industry by automating checkout processes, minimizing errors, and enhancing customer convenience. By leveraging computer vision and AI, this solution enables real-time item detection and counting, ensuring accuracy and efficiency.  </p>      Item Counting in Trolleys for Smart Shopping using Ultralytics YOLO11"},{"location":"usecases/items-counting/#hardware-model-and-dataset-information","title":"Hardware, Model, and Dataset Information","text":"<ul> <li>CPU: Intel\u00ae Core\u2122 i5-10400 CPU @ 2.90GHz.  </li> <li>GPU: NVIDIA RTX 3050 for seamless real-time processing.  </li> <li>RAM: 64 GB RAM with a 1TB Hard Disk for large-scale data handling.  </li> <li>Model: The solution utilizes a fine-tuned Ultralytics YOLO11 model, optimized for object detection and item counting.  </li> <li>Dataset: A proprietary dataset was annotated in-house, tailored specifically for this application.  </li> </ul>"},{"location":"usecases/items-counting/#real-world-applications-for-item-counting-in-retail","title":"Real-World Applications for Item Counting in Retail","text":"<ul> <li>Smart Checkouts: Automating item counting in shopping trolleys ensures fast and error-free billing, enhancing the overall customer experience.  </li> <li>Inventory Management: Provides real-time updates on stock levels, enabling retailers to optimize inventory restocking and reduce shortages.  </li> <li>Data Insights: Enables retailers to gather valuable data on customer purchasing trends, aiding in personalized marketing and inventory forecasting.  </li> </ul>"},{"location":"usecases/items-counting/#social-resources","title":"Social Resources","text":"<ul> <li>LinkedIn Post: Transforming Retail with Item Counting in Trolley using YOLO11 </li> <li>Twitter Thread: Revolutionizing Shopping Trolleys with YOLO11</li> </ul>"},{"location":"usecases/items-segmentation-supermarket-ai/","title":"Revolutionizing Supermarkets: Items Segmentation and Counting with Ultralytics YOLO11 \u2764\ufe0f\u200d\ud83d\udd25","text":"<p>Discover how to leverage the power of Ultralytics YOLO11 to achieve precise object segmentation and counting. In this guide, you'll learn step-by-step how to use YOLO11 to streamline processes, enhance accuracy, and unlock new possibilities in computer vision applications.</p>"},{"location":"usecases/items-segmentation-supermarket-ai/#system-specifications-used-to-create-this-demo","title":"System Specifications Used to Create This Demo","text":"<ul> <li>CPU: Intel\u00ae Core\u2122 i5-10400 CPU @ 2.90GHz for optimal performance.</li> <li>GPU: NVIDIA RTX 3050 for real-time processing.</li> <li>RAM &amp; Storage: 64 GB RAM and 1 TB SSD for large datasets.</li> <li>Model: Fine-tuned YOLO11 for items segmentation in trolley.</li> <li>Dataset: Custom-labeled supermarket images.</li> </ul>"},{"location":"usecases/items-segmentation-supermarket-ai/#how-to-perform-items-segmentation-and-counting","title":"How to Perform Items Segmentation and Counting","text":""},{"location":"usecases/items-segmentation-supermarket-ai/#step-1-train-or-fine-tune-yolo11","title":"Step 1: Train or Fine-Tune YOLO11","text":"<p>To get started, you can train the YOLO11 model on a custom dataset tailored to your specific use case. However, if the pre-trained YOLO11 model already performs well for your application, there's no need for customization, you can directly use the pre-trained weights for faster and efficient deployment. Explore the full details in the Ultralytics Documentation.</p>"},{"location":"usecases/items-segmentation-supermarket-ai/#step-2-how-to-draw-the-segmentation-masks","title":"Step 2: How to draw the segmentation masks","text":"<pre><code># Install ultralytics package\n# pip install ultralytics\n\nimport cv2\nfrom ultralytics import YOLO\nfrom ultralytics.utils.plotting import Annotator, colors\nfrom collections import Counter\n\nmodel = YOLO(model=\"path/to/model/file.pt\")\ncap = cv2.VideoCapture(\"path/to/video/file.mp4\")\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH,\n                                       cv2.CAP_PROP_FRAME_HEIGHT,\n                                       cv2.CAP_PROP_FPS))\n\nvwriter = cv2.VideoWriter(\"instance-segmentation4.avi\",\n                          cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\nwhile True:\n    ret, im0 = cap.read()\n    if not ret:\n        break\n\n    object_counts = Counter()  # Initialize a counter for objects detected\n    results = model.track(im0, persist=True)\n    annotator = Annotator(im0, line_width=3)\n\n    if results[0].boxes.id is not None and results[0].masks is not None:\n        masks = results[0].masks.xy\n        track_ids = results[0].boxes.id.int().cpu().tolist()\n        clss = results[0].boxes.cls.cpu().tolist()\n        boxes = results[0].boxes.xyxy.cpu()\n        for mask, box, cls, t_id in zip(masks, boxes, clss, track_ids):\n            if mask.size&gt;0:\n                object_counts[model.names[int(cls)]] += 1\n                color = colors(t_id, True)\n                mask_img = im0.copy()\n                cv2.fillPoly(mask_img, [mask.astype(int)], color)\n                cv2.addWeighted(mask_img, 0.7, im0, 1 - 0.7, 0, im0)\n                annotator.seg_bbox(mask=mask, mask_color=color,\n                                   label=str(model.names[int(cls)]),\n                                   txt_color=annotator.get_txt_color(color))\n\n    vwriter.write(im0)\n    cv2.imshow(\"Ultralytics FastSAM\", im0)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvwriter.release()\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> Fig-1: Image segmentation using YOLO11."},{"location":"usecases/items-segmentation-supermarket-ai/#step-3-count-segmented-objects","title":"Step 3: Count Segmented Objects","text":"<p>Now, we have already drawn the object masks, we can now count the objects.</p> <pre><code>import cv2\nfrom ultralytics import YOLO\nfrom ultralytics.utils.plotting import Annotator, colors\nfrom collections import Counter\n\nmodel = YOLO(model=\"path/to/model/file.pt\")\ncap = cv2.VideoCapture(\"path/to/video/file.mp4\")\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH,\n                                       cv2.CAP_PROP_FRAME_HEIGHT,\n                                       cv2.CAP_PROP_FPS))\n\nvwriter = cv2.VideoWriter(\"instance-segmentation4.avi\",\n                          cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\nwhile True:\n    ret, im0 = cap.read()\n    if not ret:\n        break\n\n    object_counts = Counter()  # Initialize a counter for objects detected\n    results = model.track(im0, persist=True)\n    annotator = Annotator(im0, line_width=3)\n\n    if results[0].boxes.id is not None and results[0].masks is not None:\n        masks = results[0].masks.xy\n        track_ids = results[0].boxes.id.int().cpu().tolist()\n        clss = results[0].boxes.cls.cpu().tolist()\n        boxes = results[0].boxes.xyxy.cpu()\n        for mask, box, cls, t_id in zip(masks, boxes, clss, track_ids):\n            if mask.size&gt;0:\n                object_counts[model.names[int(cls)]] += 1\n                color = colors(t_id, True)\n                mask_img = im0.copy()\n                cv2.fillPoly(mask_img, [mask.astype(int)], color)\n                cv2.addWeighted(mask_img, 0.7, im0, 1 - 0.7, 0, im0)\n                annotator.seg_bbox(mask=mask, mask_color=color,\n                                   label=str(model.names[int(cls)]),\n                                   txt_color=annotator.get_txt_color(color))\n\n    # Display total counts in the top-right corner\n    x, y = im0.shape[1] - 200, 30\n    margin = 10\n\n    for i, (label, count) in enumerate(object_counts.items()):\n        text = f\"{label}={count}\"\n        font_scale = 1.4\n        font_thickness = 4\n        padding = 15\n        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX,\n                                       font_scale, font_thickness)\n        rect_x2 = im0.shape[1] - 10\n        rect_x1 = rect_x2 - (text_size[0] + padding * 2)\n\n        y_position = y + i * (text_size[1] + padding * 2 + 10)\n        if y_position + text_size[1] + padding * 2 &gt; im0.shape[0]:\n            break\n        rect_y1 = y_position\n        rect_y2 = rect_y1 + text_size[1] + padding * 2\n        cv2.rectangle(im0, (rect_x1, rect_y1), (rect_x2, rect_y2),\n                      (255, 255, 255), -1)\n        text_x = rect_x1 + padding\n        text_y = rect_y1 + padding + text_size[1]\n        cv2.putText(im0, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX,\n                    font_scale, (104, 31, 17), font_thickness)\n\n    vwriter.write(im0)\n    cv2.imshow(\"Ultralytics FastSAM\", im0)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvwriter.release()\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> Fig-2: Object Counting in Shopping Trolley using YOLO11."},{"location":"usecases/items-segmentation-supermarket-ai/#applications-in-retail","title":"Applications in Retail","text":"<ul> <li>Smart Inventory Management: Categorize items and track movement effortlessly.</li> <li>Retail Analytics: Gain insights into customer behavior and product popularity.</li> </ul>"},{"location":"usecases/items-segmentation-supermarket-ai/#explore-more","title":"Explore More","text":"<ul> <li>Join the Discussion on LinkedIn </li> </ul> <p>Transform your retail operations with YOLO11 today! \ud83d\ude80</p>"},{"location":"usecases/sam2.1-vs-samurai/","title":"SAM 2.1 vs SAMURAI Tests","text":""},{"location":"usecases/segmentation-masks-detect-sam2/","title":"How to Generate Accurate Segmentation Masks Using Object Detection and SAM2 Model","text":"<p>Segmentation masks are vital for precise object tracking and analysis, allowing pixel-level identification of objects. By leveraging a fine-tuned Ultralytics YOLO11 model alongside the Segment Anything 2 (SAM2) model, you can achieve unparalleled accuracy and flexibility in your workflows.</p> Fig-1: Instance segmentation using Ultralytics YOLO11 and SAM2 model."},{"location":"usecases/segmentation-masks-detect-sam2/#hardware-and-software-setup-for-this-demo","title":"Hardware and Software Setup for This Demo","text":"<ul> <li>CPU: Intel\u00ae Core\u2122 i5-10400 CPU @ 2.90GHz for efficient processing.  </li> <li>GPU: NVIDIA RTX 3050 for real-time tasks.  </li> <li>RAM and Storage: 64 GB RAM and 1TB hard disk for seamless performance.  </li> <li>Model: Fine-tuned YOLO11 model for object detection.  </li> <li>Dataset: Custom annotated dataset for maximum accuracy.</li> </ul>"},{"location":"usecases/segmentation-masks-detect-sam2/#how-to-generate-segmentation-masks","title":"How to Generate Segmentation Masks","text":""},{"location":"usecases/segmentation-masks-detect-sam2/#step-1-prepare-the-model","title":"Step 1: Prepare the Model","text":"<p>Train or fine-tune a custom YOLO11 model, or use the Ultralytics Pretrained Models for object detection tasks.</p>"},{"location":"usecases/segmentation-masks-detect-sam2/#step-2-auto-annotation-with-sam2","title":"Step 2: Auto Annotation with SAM2","text":"<p>Integrate the SAM2 model to convert bounding boxes into segmentation masks.</p> <pre><code># Install the necessary library\n# pip install ultralytics\n\nfrom ultralytics.data.annotator import auto_annotate\n\n# Automatically annotate images using YOLO and SAM2 models\nauto_annotate(data=\"Path/to/images/directory\",\n              det_model=\"yolo11n.pt\",\n              sam_model=\"sam2_b.pt\")\n</code></pre>"},{"location":"usecases/segmentation-masks-detect-sam2/#step-3-generate-and-save-masks","title":"Step 3: Generate and Save Masks","text":"<p>Run the script to save segmentation masks as <code>.txt</code> files in the <code>images_auto_annotate_labels</code> folder.</p>"},{"location":"usecases/segmentation-masks-detect-sam2/#step-4-visualize-the-results","title":"Step 4: Visualize the Results","text":"<p>Use the following script to overlay segmentation masks on images.</p> <pre><code>import os\nimport cv2\nimport numpy as np\nfrom ultralytics.utils.plotting import colors\n\n# Define folder paths\nimage_folder = \"images_directory\"   # Path to your images directory\nmask_folder = \"images_auto_annotate_labels\" # Annotation masks directory\noutput_folder = \"output_directory\"  # Path to save output images\n\nos.makedirs(output_folder, exist_ok=True)\n\n# Process each image\nfor image_file in os.listdir(image_folder):\n    image_path = os.path.join(image_folder, image_file)\n    mask_file = os.path.join(mask_folder, \n                             os.path.splitext(image_file)[0] + \".txt\")\n\n    img = cv2.imread(image_path)   # Load the image\n    height, width, _ = img.shape\n\n    with open(mask_file, \"r\") as f:  # Read the mask file\n        lines = f.readlines()\n\n    for line in lines:\n        data = line.strip().split()\n        color = colors(int(data[0]), True)\n\n        # Convert points to absolute coordinates\n        points = np.array([(float(data[i]) * width, float(data[i + 1])*height) \n                           for i in range(1, len(data), 2)], \n                           dtype=np.int32).reshape((-1, 1, 2))\n\n        overlay = img.copy()\n        cv2.fillPoly(overlay, [points], color=color)\n        alpha = 0.6\n        cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)\n        cv2.polylines(img, [points], isClosed=True, color=color, thickness=3)\n\n    # Save the output\n    output_path = os.path.join(output_folder, image_file)\n    cv2.imwrite(output_path, img)\n    print(f\"Processed {image_file} and saved to {output_path}\")\n\nprint(\"Processing complete.\")\n</code></pre> Fig-2: Visualization of instance segmentation results. <p>That's it! After completing Step 4, you'll be able to segment objects and view the total count for each segmented object in every frame.</p>"},{"location":"usecases/segmentation-masks-detect-sam2/#real-world-applications","title":"Real-World Applications","text":"<ul> <li>Medical Imaging: Segment organs and anomalies in scans for diagnostics.  </li> <li>Retail Analytics: Detect and segment customer activities or products.  </li> <li>Robotics: Enable robots to identify objects in dynamic environments.  </li> <li>Satellite Imagery: Analyze vegetation and urban areas for planning.</li> </ul> Fig-3: Applications of instance segmentation in various fields."},{"location":"usecases/segmentation-masks-detect-sam2/#explore-more","title":"Explore More","text":"<ul> <li>Ultralytics Documentation </li> <li>Engage on LinkedIn </li> </ul> <p>Start building your object segmentation workflow today!\ud83d\ude80</p>"},{"location":"usecases/snooker-game-monitoring/","title":"Snooker Game Monitoring","text":""},{"location":"usecases/solar-panels-counting/","title":"Solar Panels Counting using Ultralytics YOLO11","text":""},{"location":"usecases/solar-panels-on-house/","title":"Solar Panels Detection on House Roof","text":""},{"location":"usecases/track-objects-in-zone/","title":"Track Objects in Zone","text":"<p>This documentation page is in progress</p>"},{"location":"usecases/traffic-counting-in-zones/","title":"Traffic Counting in Zones","text":""},{"location":"usecases/workouts-monitoring/","title":"Workouts Monitoring using Ultralytics YOLO11","text":""}]}